{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tapan/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n",
      "2225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.92500000000000004"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import *\n",
    "from pandas import Series,DataFrame\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from sklearn.preprocessing import *\n",
    "import re\n",
    "from nltk.stem import *\n",
    "from nltk.classify import *\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "from nltk.tokenize import *\n",
    "from nltk.stem import *\n",
    "from nltk.classify import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from nltk.util import *\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "tweets = []\n",
    "sentiment = []\n",
    "\n",
    "trainEndInd = 5000\n",
    "testEndInd = 8000\n",
    "\n",
    "#opening training data set file \n",
    "dataS = []\n",
    "\n",
    "# with open('subj_data_0.5.json', 'r') as jsonfile:\n",
    "#     data = json.load(jsonfile)\n",
    "#     for i in data:\n",
    "#         tweets.append(i['tweet'])\n",
    "#         sentiment.append(i['sentiment'])\n",
    "#         dataS.append((i['tweet'].strip(' \\n\\\"'), i['sentiment'].strip(' \\n\\\"')))\n",
    "                     \n",
    "                     \n",
    "    \n",
    "# with open('data.csv', 'r', encoding='utf-8', errors='ignore') as csvfile:\n",
    "#     tweetData = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "#     #retreiving the tweet and the sentiment \n",
    "#     for row in tweetData:\n",
    "#         sentiment.append(row[0].strip(' \\n\\\"'))\n",
    "#         tweets.append(row[5].strip(' \\n\\\"'))\n",
    "#         dataS.append((row[0].strip(' \\n\\\"'), row[5].strip(' \\n\n",
    "\n",
    "filetext = open('datatext.txt', 'r')\n",
    "filelabel = open('datalabel.txt', 'r')\n",
    "\n",
    "for lines in filetext.readlines():\n",
    "    tweets.append(lines)\n",
    "\n",
    "for labels in filelabel.readlines():\n",
    "    sentiment.append(int(labels))\n",
    "\n",
    "random.shuffle(dataS)\n",
    "\n",
    "# tweets = []\n",
    "# sentiment = []\n",
    "# datasize = 20000\n",
    "# for x in dataS:\n",
    "#     if len(tweets) > datasize:\n",
    "#         break\n",
    "#     tweets.append(x[0])\n",
    "#     sentiment.append(x[1])\n",
    "tweets = np.array(tweets)\n",
    "sentiment = np.array(sentiment)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, sentiment, random_state=0, test_size=0)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmer1 = SnowballStemmer(\"english\")\n",
    "\n",
    "precessedData = []\n",
    "stopWords = []\n",
    "\n",
    "#contains stop words to be removed\n",
    "fp = open('StopWords.txt', 'r')\n",
    "line = fp.readline()\n",
    "while line:\n",
    "    word = line.strip()\n",
    "    stopWords.append(word)\n",
    "    line = fp.readline()\n",
    "fp.close()\n",
    "\n",
    "#storing the acronym dictionary\n",
    "acroDict1 = []\n",
    "acroDict2 = []\n",
    "fp2= open('AcronymDict.txt', 'r')\n",
    "line = fp2.readline()\n",
    "while line:\n",
    "    synonyms = line.split('-')\n",
    "    acroDict1.append(synonyms[0])\n",
    "    acroDict2.append(synonyms[1])\n",
    "    line = fp2.readline()\n",
    "fp2.close()\n",
    "\n",
    "#starting the function \n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end\n",
    "\n",
    "def preprocessTweet(Tweet):\n",
    "    Tweet = re.sub('((www\\.[\\s]+)|(https?://[^\\s]+))',' ',Tweet) #strip off URLs\n",
    "    Tweet = re.sub('@[^\\s]+','',Tweet) #removing user tags\n",
    "    Tweet = re.sub(r'#([^\\s]+)', r'\\1', Tweet) #replacing hash tag followed by word with just the word\n",
    "    Tweet = replaceTwoOrMore(Tweet) #look for 2 or more repetitions of character and replace with the character itself \n",
    "    Tweet = Tweet.strip(' \\n,;:-?!*()\\\"') #removing punctuations\n",
    "    Tweet = re.sub('[&*!-?$#^.,:;%<>}{[]/\\\"]',' ',Tweet) #removing symbols\n",
    "    Tweet = re.sub('[\\s]+','', Tweet) #extra whitespaces handled\n",
    "    Tweet = Tweet.lower() #convert Tweet to lower case\n",
    "    a = ':)' \n",
    "    b = ':('\n",
    "    #stripping of emoticons\n",
    "    Tweet = Tweet.replace(a,'')\n",
    "    Tweet = Tweet.replace(b,'')\n",
    "    #performing stemming\n",
    "    #Tweet = stemmer.stem(Tweet)\n",
    "    #Tweet = stemmer1.stem(Tweet)\n",
    "    if Tweet in stopWords:\n",
    "        return ''\n",
    "    if len(Tweet) > 0:\n",
    "        return Tweet\n",
    "    return ''\n",
    "\n",
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    This function removes stop words and punctuation with the NLTK tokenize package required for Part B\n",
    "    \"\"\"\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.strip('{,\\\" \\n')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    filtered_words = [w.lower() for w in tokens]\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "processedData = []\n",
    "traintweet = []\n",
    "for Tweet1 in X_train:\n",
    "    Tweet1 = re.sub('\\.\\.+', ' ', Tweet1)\n",
    "    Tweet1 = re.sub('\\-\\-+', ' ', Tweet1)\n",
    "    Tweet1 = re.sub('\\.', '', Tweet1) \n",
    "    Tweet1 = preprocess(Tweet1)\n",
    "    str = \"\"\n",
    "    for Tweet in Tweet1.split():\n",
    "        Tweet = preprocessTweet(Tweet)\n",
    "        if len(Tweet) > 0:\n",
    "            str = str + Tweet + ' '\n",
    "    processedData.append(str)\n",
    "    traintweet.append(str)\n",
    "\n",
    "#Dont't Run\n",
    "print(len(y_train))\n",
    "print(len(processedData))\n",
    "\n",
    "tweets = np.array(processedData)\n",
    "sentiment = np.array(y_train)\n",
    "\n",
    "#tweets, X_test, sentiment, y_test = train_test_split(tweets, sentiment, random_state=2, test_size=0)\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range = (1, 1), max_features=1000)\n",
    "X_new_counts = count_vect.fit_transform(tweets)\n",
    "#tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "#X_train_tfidf = tfidf_transformer.fit_transform(X_new_counts)\n",
    "#X_train_tfidf = X_new_counts\n",
    "clf = NuSVC().fit(X_new_counts[:800], sentiment[:800])\n",
    "#clf = RandomForestClassifier(100).fit(X_new_counts[:40000], sentiment[:40000])\n",
    "#clf = MultinomialNB().fit(X_new_counts[:1000], sentiment[:1000])\n",
    "#clf = LogisticRegression().fit(X_new_counts[:9000], sentiment[:9000])\n",
    "predicted= clf.predict(X_new_counts[800:1000])\n",
    "np.mean(predicted == sentiment[800:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)\n",
      "(0, 1000)\n"
     ]
    }
   ],
   "source": [
    "import GPflow\n",
    "from __future__ import print_function\n",
    "import GPflow\n",
    "from GPflow import ekernels\n",
    "from GPflow import kernels\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#import pods\n",
    "#pods.datasets.overide_manual_authorize = True  # dont ask to authorize\n",
    "np.random.seed(42)\n",
    "GPflow.settings.numerics.quadrature = 'error'  # throw error if quadrature is used for kernel expectations\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_new_counts[:1000], sentiment[:1000], random_state=2, test_size=0)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "model3=RandomForestClassifier(100).fit(x_train[:800],y_train[:800])\n",
    "accuracy_score(model3.predict(x_train[800:1000]),y_train[800:1000])\n",
    "\n",
    "Y = x_train[:1000].toarray()\n",
    "Y = Y.astype(float)\n",
    "Q = 100\n",
    "M = 200  # number of inducing pts\n",
    "N = Y.shape[0]\n",
    "X_mean = GPflow.gplvm.PCA_reduce(Y, Q) # Initialise via PCA\n",
    "\n",
    "Z = np.random.permutation(X_mean.copy())[:M]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = ekernels.Linear(100, ARD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = GPflow.gplvm.BayesianGPLVM(X_mean=X_mean, X_var=0.1*np.ones((N, Q)), Y=Y,\n",
    "                                kern=k, M=M, Z=Z)\n",
    "m.likelihood.variance = 0.01\n",
    "m.optimize(disp=True, maxiter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
