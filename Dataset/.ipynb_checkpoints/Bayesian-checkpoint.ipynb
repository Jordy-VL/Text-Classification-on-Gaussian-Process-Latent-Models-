{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import *\n",
    "from pandas import Series,DataFrame\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from sklearn.preprocessing import *\n",
    "import re\n",
    "from nltk.stem import *\n",
    "from nltk.classify import *\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "from nltk.tokenize import *\n",
    "from nltk.stem import *\n",
    "from nltk.classify import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from nltk.util import *\n",
    "import re\n",
    "import math\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571864\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "sentiment = []\n",
    "\n",
    "trainEndInd = 5000\n",
    "testEndInd = 8000\n",
    "\n",
    "#opening training data set file \n",
    "\n",
    "with open('subj_data_0.5.json', 'r') as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "    for i in data:\n",
    "        tweets.append(i['tweet'])\n",
    "        sentiment.append(i['sentiment'])\n",
    "\n",
    "# with open('data.csv', 'r', encoding='utf-8', errors='ignore') as csvfile:\n",
    "#     tweetData = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "#     #retreiving the tweet and the sentiment \n",
    "#     for row in tweetData:\n",
    "#         sentiment.append(row[0].strip(' \\n\\\"'))\n",
    "#         tweets.append(row[5])\n",
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = np.array(tweets)\n",
    "sentiment = np.array(sentiment)\n",
    "\n",
    "#shuffle data set so as to remove redundant sentiments\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, sentiment, random_state=69, test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer1 = SnowballStemmer(\"english\")\n",
    "\n",
    "precessedData = []\n",
    "stopWords = []\n",
    "\n",
    "#contains stop words to be removed\n",
    "fp = open('StopWords.txt', 'r')\n",
    "line = fp.readline()\n",
    "while line:\n",
    "    word = line.strip()\n",
    "    stopWords.append(word)\n",
    "    line = fp.readline()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#storing the acronym dictionary\n",
    "acroDict1 = []\n",
    "acroDict2 = []\n",
    "fp2= open('AcronymDict.txt', 'r')\n",
    "line = fp2.readline()\n",
    "while line:\n",
    "    synonyms = line.split('-')\n",
    "    acroDict1.append(synonyms[0])\n",
    "    acroDict2.append(synonyms[1])\n",
    "    line = fp2.readline()\n",
    "fp2.close()\n",
    "\n",
    "posnegDict = []\n",
    "fp3 = open('AFINN.txt', 'r')\n",
    "line = fp3.readline()\n",
    "while line:\n",
    "    scores = line.split(' ')\n",
    "    posnegDict.append((scores[0],scores[1]))\n",
    "    synSet = wordnet.synsets(scores[0])\n",
    "    for i in synSet:\n",
    "        posnegDict.append((i.name().split(\".\")[0],scores[1]))\n",
    "    line = fp3.readline()\n",
    "fp3.close()\n",
    "\n",
    "fp3 = open('Negation.txt', 'r')\n",
    "line = fp3.readline()\n",
    "neg = []\n",
    "while line:\n",
    "    line = line.strip(' \\n')\n",
    "    neg.append(line.lower())\n",
    "    line = fp3.readline()\n",
    "fp3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#starting the function \n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end\n",
    "\n",
    "def preprocessTweet(Tweet):\n",
    "    Tweet = re.sub('((www\\.[\\s]+)|(https?://[^\\s]+))',' ',Tweet) #strip off URLs\n",
    "    Tweet = re.sub('@[^\\s]+','',Tweet) #removing user tags\n",
    "    Tweet = re.sub(r'#([^\\s]+)', r'\\1', Tweet) #replacing hash tag followed by word with just the word\n",
    "    Tweet = replaceTwoOrMore(Tweet) #look for 2 or more repetitions of character and replace with the character itself \n",
    "    Tweet = Tweet.strip(' \\n,;:-?!*()\\\"') #removing punctuations\n",
    "    Tweet = re.sub('[&*!-?$#^.,:;%<>}{[]/\\\"]',' ',Tweet) #removing symbols\n",
    "    Tweet = re.sub('[\\s]+','', Tweet) #extra whitespaces handled\n",
    "    Tweet = Tweet.lower() #convert Tweet to lower case\n",
    "    a = ':)' \n",
    "    b = ':('\n",
    "    #stripping of emoticons\n",
    "    Tweet = Tweet.replace(a,'')\n",
    "    Tweet = Tweet.replace(b,'')\n",
    "    #performing stemming\n",
    "    #Tweet = stemmer.stem(Tweet)\n",
    "    #Tweet = stemmer1.stem(Tweet)\n",
    "    if Tweet in stopWords:\n",
    "        return ''\n",
    "    if len(Tweet) > 0:\n",
    "        return Tweet\n",
    "    return ''\n",
    "\n",
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    This function removes stop words and punctuation with the NLTK tokenize package required for Part B\n",
    "    \"\"\"\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.strip('{,\\\" \\n')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    filtered_words = [w.lower() for w in tokens]\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "processedData = []\n",
    "traintweet = []\n",
    "for Tweet1 in X_train[:100000]:\n",
    "    Tweet1 = re.sub('\\.\\.+', ' ', Tweet1)\n",
    "    Tweet1 = re.sub('\\-\\-+', ' ', Tweet1)\n",
    "    Tweet1 = re.sub('\\.', '', Tweet1) \n",
    "    Tweet1 = preprocess(Tweet1)\n",
    "    str = \"\"\n",
    "    for Tweet in Tweet1.split():\n",
    "        if Tweet in acroDict1:#if tweet is an acronym\n",
    "            Tweet = acroDict2[acroDict1.index(Tweet)]#replace it with its expanded form from the acronym dictionary\n",
    "            for Tweet2 in Tweet.split(' '):#split the words again now\n",
    "                j = preprocessTweet(Tweet2)\n",
    "                if j != '':\n",
    "                    str = str + j + ' '\n",
    "            continue\n",
    "        Tweet = preprocessTweet(Tweet)\n",
    "        if Tweet in neg:\n",
    "            Tweet = 'not'\n",
    "        if len(Tweet) > 0:\n",
    "            str = str + Tweet + ' '\n",
    "    processedData.append(str)\n",
    "    traintweet.append(str)\n",
    "print(len(processedData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93569999999999998"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dont't Run\n",
    "tweets = np.array(processedData)\n",
    "trainT = np.array(traintweet)\n",
    "sentiment = np.array(y_train)\n",
    "count_vect = CountVectorizer(ngram_range = (1, 2))\n",
    "X_new_counts = count_vect.fit_transform(tweets)\n",
    "#tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "#X_train_tfidf = tfidf_transformer.fit_transform(X_new_counts)\n",
    "#X_train_tfidf = X_new_counts\n",
    "#clf = NuSVC().fit(X_new_counts[:40000], sentiment[:40000])\n",
    "#clf = RandomForestClassifier(100).fit(X_new_counts[:40000], sentiment[:40000])\n",
    "clf = MultinomialNB().fit(X_new_counts[:100000], sentiment[:100000])\n",
    "#clf = LogisticRegression().fit(X_new_counts[:90000], sentiment[:90000])\n",
    "predicted= clf.predict(X_new_counts[90000:100000])\n",
    "np.mean(predicted == sentiment[90000:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "#Dont't Run\n",
    "\n",
    "\n",
    "# Feature vector for unigrams\n",
    "def unigramFeatureVector(tweet):\n",
    "  featureVector = []\n",
    "  #split tweet into words\n",
    "  words = tweet.split()\n",
    "  for w in words:\n",
    "    val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "    if val is None:\n",
    "      continue\n",
    "    else:\n",
    "      featureVector.append(w)\n",
    "  return featureVector\n",
    "\n",
    "# Feature vector for Bigrams\n",
    "def bigramFeatureVector(tweet):\n",
    "  featureVector = []\n",
    "  #split tweet into words\n",
    "  words = tweet.split()\n",
    "  for i in range(len(words)-1):\n",
    "    val1 = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", words[i])\n",
    "    val2 = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", words[i+1])\n",
    "    if (val1 is None or val2 is None):\n",
    "      continue\n",
    "    else:\n",
    "      featureVector.append(words[i] + \" \" + words[i+1])\n",
    "  return featureVector\n",
    "#end\n",
    "\n",
    "# Feature vector with unigrams and bigrams\n",
    "def unigramBigramFeatureVector(tweet):\n",
    "  # get unigrams\n",
    "  unigrams = unigramFeatureVector(tweet)\n",
    "  # get bigrams\n",
    "  bigrams = bigramFeatureVector(tweet)\n",
    "  return unigrams + bigrams\n",
    "#end\n",
    "\n",
    "def unigramWithPosTag(tweet):\n",
    "  #_POS_TAGGER='taggers/maxent_treebank_pos_tagger/english.pickle'\n",
    "  #tagger=load(_POS_TAGGER)\n",
    "  posTweet=nltk.word_tokenize(tweet)\n",
    "  return nltk.pos_tag(posTweet)\n",
    "  #return tagger.tag(posTweet)\n",
    "#end\n",
    "\n",
    "def unigramWithAveragedPerceptronPosTag(tweet):\n",
    "  posTweet = nltk.word_tokenize(tweet)\n",
    "  tagger=PerceptronTagger()\n",
    "  return tagger.tag(posTweet)\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dont't Run\n",
    "\n",
    "\n",
    "Tweets = []\n",
    "k = 0\n",
    "for row in tweets:\n",
    "  Tweets.append((unigramWithAveragedPerceptronPosTag(row), sentiment[k]))\n",
    "  #Tweets.append((unigramWithPosTag(row), sentiment[k]))\n",
    "  k = k + 1\n",
    "    \n",
    "tweets = Tweets\n",
    "\n",
    "def get_words_in_tweets(tweets):\n",
    "  all_words = []\n",
    "  for (text, sentiment) in tweets:\n",
    "    all_words.extend(text)\n",
    "  return all_words\n",
    "\n",
    "def get_word_features(wordlist):\n",
    "    \n",
    "  # This line calculates the frequency distrubtion of all words in tweets\n",
    "  wordlist = nltk.FreqDist(wordlist)\n",
    "  word_features = wordlist.keys()\n",
    "  \n",
    "  # This prints out the list of all distinct words in the text in order\n",
    "  # of their number of occurrences.\n",
    "  return word_features\n",
    "\n",
    "word_features = get_word_features(get_words_in_tweets(tweets)) #our list of many words \n",
    "\n",
    "\n",
    "def extract_features(tweet):\n",
    "  settweet = set(tweet)\n",
    "  features = {}\n",
    "  for word in word_features:\n",
    "    #features['contains(%s)' % word] = (word in settweet)\n",
    "    features['contains({0})'.format(word)] = (word in settweet)\n",
    "  return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dont't Run\n",
    "\n",
    "\n",
    "print ('Training...')\n",
    "\n",
    "#training on tuples numbered 1 to 1000 and testing on 1001 to 1100\n",
    "training_set = nltk.classify.apply_features(extract_features, tweets[:100])\n",
    "test_set = nltk.classify.apply_features(extract_features, tweets[101:201])\n",
    "\n",
    "def accuracy(classifier, gold):\n",
    "    results = classifier.classify_many([fs for (fs, l) in gold])\n",
    "    correct = [l == r for ((fs, l), r) in zip(gold, results)]\n",
    "    if correct:\n",
    "        return sum(correct) / len(correct)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#classifier specified for training\n",
    "#classifier = naivebayes.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "#classifier = nltk.classify.SklearnClassifier(NuSVC())\n",
    "#classifier = nltk.classify.SklearnClassifier(LinearSVC())\n",
    "#classifier.train(training_set)\n",
    "\n",
    "#classifier = maxent.MaxentClassifier.train(training_set)\n",
    "\n",
    "#classifier = decisiontree.DecisionTreeClassifier.train(training_set)\n",
    "\n",
    "#classifier = nltk.classify.SklearnClassifier(RandomForestClassifier(100))\n",
    "classifierNB = naivebayes.NaiveBayesClassifier.train(training_set)\n",
    "classifierRF = nltk.classify.SklearnClassifier(RandomForestClassifier(100))\n",
    "classifierRF.train(training_set)\n",
    "classifierSVC = nltk.classify.SklearnClassifier(NuSVC())\n",
    "classifierSVC.train(training_set)\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n",
    "    \n",
    "voted_classifier = VoteClassifier(classifierNB,\n",
    "                                 classifierRF,\n",
    "                                 classifierSVC)\n",
    "    \n",
    "print('Testing...')\n",
    "print(\"Naive Bayes classifier accuracy percent:\", (nltk.classify.accuracy(classifierNB, test_set))*100)\n",
    "print(\"Random Forest accuracy percent:\", (nltk.classify.accuracy(classifierRF, test_set))*100)\n",
    "print(\"SVM accuracy percent:\", (nltk.classify.accuracy(classifierSVC, test_set))*100)\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, test_set))*100)\n",
    "# print ('Testing.....')\n",
    "# accuracyP = accuracy(classifierEns,test_set) \n",
    "# print (accuracyP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359\n"
     ]
    }
   ],
   "source": [
    "testT = []\n",
    "testS = []\n",
    "with open('testdata.csv', 'r', encoding='utf-8', errors='ignore') as csvfile:\n",
    "    tweetData = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    #retreiving the tweet and the sentiment \n",
    "    for row in tweetData:\n",
    "        if (row[0].strip(' \\n,\\t.\\\"')) == \"2\":\n",
    "            continue\n",
    "        testS.append(row[0].strip(' \\n'))\n",
    "        testT.append(row[5])\n",
    "print(len(testS))\n",
    "testarr = np.array(testS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100359\n"
     ]
    }
   ],
   "source": [
    "testData = []\n",
    "testLabel = []\n",
    "k = 0 \n",
    "for Tweet1 in testT:\n",
    "    Tweet1 = re.sub('\\.\\.+', ' ', Tweet1)\n",
    "    Tweet1 = re.sub('\\-\\-+', ' ', Tweet1)\n",
    "    Tweet1 = re.sub('\\.', '', Tweet1) \n",
    "    #Tweet1 = preprocess(Tweet1)\n",
    "    str = \"\"\n",
    "    for Tweet in Tweet1.split():\n",
    "        if Tweet in acroDict1:#if tweet is an acronym\n",
    "            Tweet = acroDict2[acroDict1.index(Tweet)]#replace it with its expanded form from the acronym dictionary\n",
    "            for Tweet2 in Tweet.split(' '):#split the words again now\n",
    "                j = preprocessTweet(Tweet2)\n",
    "                if j != '':\n",
    "#                     if Tweet == \"lol\":\n",
    "#                         print(j)\n",
    "                    str = str + j + ' '\n",
    "            continue\n",
    "        Tweet = preprocessTweet(Tweet)\n",
    "        if Tweet in neg:\n",
    "            Tweet = 'not'\n",
    "        if len(Tweet) > 0:\n",
    "            str = str + Tweet + ' '\n",
    "    testData.append(str)\n",
    "    testLabel.append(testS[k])\n",
    "    k += 1\n",
    "print(len(traintweet))\n",
    "processedData = traintweet\n",
    "for tweet in testData:\n",
    "    processedData.append(tweet)\n",
    "print(len(processedData))\n",
    "sentimentT = []\n",
    "for label in testLabel:\n",
    "    sentimentT.append(label)\n",
    "sentimentT = np.array(sentimentT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "(359,)\n",
      "(100359,)\n",
      "lyokog hello wa realli nice thi morn appear gone overcast \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75208913649025066"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLenght = 100000\n",
    "print(trainLenght)\n",
    "print(sentimentT.shape)\n",
    "sentiment = np.concatenate((sentiment[:trainLenght], sentimentT), axis = 0)\n",
    "print(sentiment.shape)\n",
    "tweets = np.array(processedData)\n",
    "print(tweets[0])\n",
    "sentiment = np.array(y_train)\n",
    "count_vect = CountVectorizer(ngram_range = (1, 1))\n",
    "X_new_counts = count_vect.fit_transform(tweets)\n",
    "#tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "#X_train_tfidf = tfidf_transformer.fit_transform(X_new_counts)\n",
    "#X_train_tfidf = X_new_counts\n",
    "#clf = NuSVC().fit(X_new_counts[:40000], sentiment[:40000])\n",
    "#clf = RandomForestClassifier(100).fit(X_new_counts[:40000], sentiment[:40000])\n",
    "clf = MultinomialNB().fit(X_new_counts[:trainLenght], sentiment[:trainLenght])\n",
    "#clf = LogisticRegression().fit(X_new_counts[:trainLenght], sentiment[:trainLenght])\n",
    "predicted= clf.predict(X_new_counts[100000:100000 + 359])\n",
    "np.mean(predicted == testarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  Word2Vec model...\n",
      "Trained Word2Vec Model\n"
     ]
    }
   ],
   "source": [
    "def getList(processedData):\n",
    "    l = []\n",
    "    for tweet in processedData:\n",
    "        f = [x for x in tweet.split()]\n",
    "        l.append([f])\n",
    "    return l\n",
    "newcorpus = getList(processedData)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(oorpus, sentiment, random_state=12, test_size=0.2)\n",
    "corpusTw = []\n",
    "for item in newcorpus:\n",
    "    corpusTw += item\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "#Initialize and train the model (this will take some time)\n",
    "print (\"Training  Word2Vec model...\")\n",
    "model = word2vec.Word2Vec(corpusTw, workers=num_workers, \\\n",
    "            size = num_features, min_count = 1, \\\n",
    "            window = context, sample = downsampling)\n",
    "print(\"Trained Word2Vec Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildWordVector(text, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += model[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "train_vecs = np.concatenate([buildWordVector(z, num_features) for z in processedData])\n",
    "train_vecs = scale(train_vecs)\n",
    "y_train = sentiment[:len(train_vecs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: '\"0\"'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9355441552ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Tapan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_scaling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m                 self.max_iter, self.tol, self.random_state)\n\u001b[0m\u001b[1;32m   1155\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tapan/anaconda/lib/python3.5/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon)\u001b[0m\n\u001b[1;32m    883\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[1;32m    884\u001b[0m                              \u001b[0;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: '\"0\"'"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vecs[:100000], y_train[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LogisticRegression instance is not fitted yet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-264bb159f27e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tapan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Tapan/anaconda/lib/python3.5/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coef_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             raise NotFittedError(\"This %(name)s instance is not fitted \"\n\u001b[0;32m--> 242\u001b[0;31m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This LogisticRegression instance is not fitted yet"
     ]
    }
   ],
   "source": [
    "predicted= lr.predict(train_vecs[:100000])\n",
    "np.mean(predicted == y_train[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shit', 0.9842032194137573),\n",
       " ('blogtv', 0.9830363392829895),\n",
       " ('scared', 0.9826571345329285),\n",
       " ('crying', 0.9793097972869873),\n",
       " ('f**k', 0.9791483283042908),\n",
       " ('fucking', 0.9785919785499573),\n",
       " ('somethings', 0.9784562587738037),\n",
       " ('mad', 0.9769027233123779),\n",
       " ('lame', 0.9761031866073608),\n",
       " ('anymore', 0.9749095439910889)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
