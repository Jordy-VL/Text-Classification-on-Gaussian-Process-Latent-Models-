{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import *\n",
    "from pandas import Series,DataFrame\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from sklearn.preprocessing import *\n",
    "import re\n",
    "from nltk.stem import *\n",
    "from nltk.classify import *\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "from nltk.tokenize import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import *\n",
    "from nltk.classify import *\n",
    "from sklearn.feature_extraction.text import *\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from nltk.util import *\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tweets = []\n",
    "# sentiment = []\n",
    "\n",
    "# trainEndInd = 5000\n",
    "# testEndInd = 8000\n",
    "\n",
    "# #opening training data set file \n",
    "# dataS = []\n",
    "\n",
    "# with open('subj_data_0.5.json', 'r') as jsonfile:\n",
    "#     data = json.load(jsonfile)\n",
    "#     for i in data:\n",
    "#         tweets.append(i['tweet'])\n",
    "#         sentiment.append(i['sentiment'])\n",
    "#         dataS.append((i['tweet'].strip(' \\n\\\"'), i['sentiment'].strip(' \\n\\\"')))\n",
    "                     \n",
    "                     \n",
    "    \n",
    "# with open('data.csv', 'r', encoding='utf-8', errors='ignore') as csvfile:\n",
    "#     tweetData = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "#     #retreiving the tweet and the sentiment \n",
    "#     for row in tweetData:\n",
    "#         sentiment.append(row[0].strip(' \\n\\\"'))\n",
    "#         tweets.append(row[5].strip(' \\n\\\"'))\n",
    "#         dataS.append((row[0].strip(' \\n\\\"'), row[5].strip(' \\n\n",
    "\n",
    "tweets = []\n",
    "sentiment = []\n",
    "dataS = []\n",
    "filetext = open('datatext.txt', 'r')\n",
    "filelabel = open('datalabel.txt', 'r')\n",
    "\n",
    "for lines in filetext.readlines():\n",
    "    tweets.append(lines)\n",
    "\n",
    "for labels in filelabel.readlines():\n",
    "    sentiment.append(int(labels))\n",
    "\n",
    "random.shuffle(dataS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tweets = []\n",
    "# sentiment = []\n",
    "# datasize = 20000\n",
    "# for x in dataS:\n",
    "#     if len(tweets) > datasize:\n",
    "#         break\n",
    "#     tweets.append(x[0])\n",
    "#     sentiment.append(x[1])\n",
    "tweets = np.array(tweets)\n",
    "sentiment = np.array(sentiment)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets, sentiment, random_state=0, test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer1 = SnowballStemmer(\"english\")\n",
    "\n",
    "precessedData = []\n",
    "stopWords = []\n",
    "\n",
    "#contains stop words to be removed\n",
    "fp = open('StopWords.txt', 'r')\n",
    "line = fp.readline()\n",
    "while line:\n",
    "    word = line.strip()\n",
    "    stopWords.append(word)\n",
    "    line = fp.readline()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#storing the acronym dictionary\n",
    "acroDict1 = []\n",
    "acroDict2 = []\n",
    "fp2= open('AcronymDict.txt', 'r')\n",
    "line = fp2.readline()\n",
    "while line:\n",
    "    synonyms = line.split('-')\n",
    "    acroDict1.append(synonyms[0])\n",
    "    acroDict2.append(synonyms[1])\n",
    "    line = fp2.readline()\n",
    "fp2.close()\n",
    "\n",
    "posnegDict = []\n",
    "fp3 = open('AFINN.txt', 'r')\n",
    "line = fp3.readline()\n",
    "while line:\n",
    "    scores = line.split(' ')\n",
    "    posnegDict.append((scores[0],scores[1]))\n",
    "    synSet = wordnet.synsets(scores[0])\n",
    "    for i in synSet:\n",
    "        posnegDict.append((i.name().split(\".\")[0],scores[1]))\n",
    "    line = fp3.readline()\n",
    "fp3.close()\n",
    "\n",
    "fp3 = open('Negation.txt', 'r')\n",
    "line = fp3.readline()\n",
    "neg = []\n",
    "while line:\n",
    "    line = line.strip(' \\n')\n",
    "    neg.append(line.lower())\n",
    "    line = fp3.readline()\n",
    "fp3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#starting the function \n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "#end\n",
    "\n",
    "def preprocessTweet(Tweet):\n",
    "    Tweet = re.sub('((www\\.[\\s]+)|(https?://[^\\s]+))',' ',Tweet) #strip off URLs\n",
    "    Tweet = re.sub('@[^\\s]+','',Tweet) #removing user tags\n",
    "    Tweet = re.sub(r'#([^\\s]+)', r'\\1', Tweet) #replacing hash tag followed by word with just the word\n",
    "    Tweet = replaceTwoOrMore(Tweet) #look for 2 or more repetitions of character and replace with the character itself \n",
    "    Tweet = Tweet.strip(' \\n,;:-?!*()\\\"') #removing punctuations\n",
    "    Tweet = re.sub('[&*!-?$#^.,:;%<>}{[]/\\\"]',' ',Tweet) #removing symbols\n",
    "    Tweet = re.sub('[\\s]+','', Tweet) #extra whitespaces handled\n",
    "    Tweet = Tweet.lower() #convert Tweet to lower case\n",
    "    a = ':)' \n",
    "    b = ':('\n",
    "    #stripping of emoticons\n",
    "    Tweet = Tweet.replace(a,'')\n",
    "    Tweet = Tweet.replace(b,'')\n",
    "    #performing stemming\n",
    "    #Tweet = stemmer.stem(Tweet)\n",
    "    #Tweet = stemmer1.stem(Tweet)\n",
    "    if Tweet in stopWords:\n",
    "        return ''\n",
    "    if len(Tweet) > 0:\n",
    "        return Tweet\n",
    "    return ''\n",
    "\n",
    "def preprocess(tweet):\n",
    "    \"\"\"\n",
    "    This function removes stop words and punctuation with the NLTK tokenize package required for Part B\n",
    "    \"\"\"\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.strip('{,\\\" \\n')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    filtered_words = [w.lower() for w in tokens]\n",
    "    filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "processedData = []\n",
    "traintweet = []\n",
    "for Tweet1 in X_train:\n",
    "    Tweet1 = re.sub('\\.\\.+', ' ', Tweet1)\n",
    "    Tweet1 = re.sub('\\-\\-+', ' ', Tweet1)\n",
    "    Tweet1 = re.sub('\\.', '', Tweet1) \n",
    "    Tweet1 = preprocess(Tweet1)\n",
    "    str = \"\"\n",
    "    for Tweet in Tweet1.split():\n",
    "        Tweet = preprocessTweet(Tweet)\n",
    "        if len(Tweet) > 0:\n",
    "            str = str + Tweet + ' '\n",
    "    processedData.append(str)\n",
    "    traintweet.append(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Dont't Run\n",
    "print(len(y_train))\n",
    "print(len(processedData))\n",
    "\n",
    "tweets = np.array(processedData)\n",
    "sentiment = np.array(y_train)\n",
    "\n",
    "\n",
    "\n",
    "#tweets, X_test, sentiment, y_test = train_test_split(tweets, sentiment, random_state=2, test_size=0)\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer(ngram_range = (1, 1), max_features=1000)\n",
    "X_new_counts = count_vect.fit_transform(tweets)\n",
    "#tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "#X_train_tfidf = tfidf_transformer.fit_transform(X_new_counts)\n",
    "#X_train_tfidf = X_new_counts\n",
    "#clf = NuSVC().fit(X_new_counts[:800], sentiment[:800])\n",
    "clf = RandomForestClassifier(100).fit(X_new_counts[:800], sentiment[:800])\n",
    "#clf = MultinomialNB().fit(X_new_counts[:1000], sentiment[:1000])\n",
    "#clf = LogisticRegression().fit(X_new_counts[:9000], sentiment[:9000])\n",
    "predicted= clf.predict(X_new_counts[800:1000])\n",
    "np.mean(predicted == sentiment[800:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(X_new_counts[:9000].toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import GPflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import GPflow\n",
    "from GPflow import ekernels\n",
    "from GPflow import kernels\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "#import pods\n",
    "#pods.datasets.overide_manual_authorize = True  # dont ask to authorize\n",
    "np.random.seed(42)\n",
    "GPflow.settings.numerics.quadrature = 'error'  # throw error if quadrature is used for kernel expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_trainred, x_testred, y_trainred, y_testred = train_test_split(X_new_counts[:2000], sentiment[:2000], random_state=2, test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_new_counts[:1000], sentiment[:1000], random_state=2, test_size=0)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "model3=RandomForestClassifier(100).fit(x_train[:800],y_train[:800])\n",
    "accuracy_score(model3.predict(x_train[800:1000]),y_train[800:1000])\n",
    "\n",
    "Y = x_train[:1000].toarray()\n",
    "Y = Y.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q = 100\n",
    "M = 200  # number of inducing pts\n",
    "\n",
    "N = Y.shape[0]\n",
    "X_mean = GPflow.gplvm.PCA_reduce(Y, Q) # Initialise via PCA\n",
    "\n",
    "Z = np.random.permutation(X_mean.copy())[:M]\n",
    "# fHmmm = False\n",
    "# if(fHmmm):\n",
    "#     k = ekernels.Add([ekernels.RBF(3, ARD=True, active_dims=slice(0,3)),\n",
    "#                   ekernels.Linear(2, ARD=False, active_dims=slice(3,5))])\n",
    "# else:\n",
    "#     k = ekernels.Add([ekernels.RBF(3, ARD=True, active_dims=[0,1, 2]),\n",
    "#                   ekernels.Linear(2, ARD=False, active_dims=[3, 4])])\n",
    "\n",
    "dimension = [x for x in range(0, 100)]\n",
    "k = ekernels.RBF(20, ARD=False)\n",
    "\n",
    "m = GPy.models.BayesianGPLVM(X_mean=X_mean, X_var=0.1*np.ones((N, Q)), Y=Y,\n",
    "                                kern=k, M=M, Z=Z)\n",
    "m.likelihood.variance = 0.01\n",
    "m.optimize(disp=True, maxiter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print((np.array(m.X_mean.value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "XPCAplot = GPflow.gplvm.PCA_reduce(Y, 2)\n",
    "f, ax = plt.subplots(1,2, figsize=(10,6))\n",
    "labels=np.array(y_trainred)\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(np.unique(labels))))\n",
    "\n",
    "for i, c in zip(np.unique(labels), colors):\n",
    "    ax[0].scatter(XPCAplot[labels==i,0], XPCAplot[labels==i,1], color=c, label=i)\n",
    "    ax[0].set_title('PCA')\n",
    "    ax[1].scatter(m.X_mean.value[labels==i,1], m.X_mean.value[labels==i,2], color=c, label=i)\n",
    "    ax[1].set_title('Bayesian GPLVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# model = svm.LinearSVC()\n",
    "# model.fit(np.array(m.X_mean.value[:3500]),y_train[:3500])\n",
    "# accuracy_score(model.predict(m.X_mean.value[3500:4500]),y_train[3500:4500])\n",
    "\n",
    "model2 = svm.NuSVC()\n",
    "model2.fit(np.array(m.X_mean.value[:800]),np.array(y_trainred[:800]))\n",
    "accuracy_score(model2.predict(m.X_mean.value[800:1000]),y_trainred[800:1000])\n",
    "\n",
    "\n",
    "\n",
    "# model3=RandomForestClassifier(100).fit(np.array(m.X_mean.value[:3500]),y_train[:3500])\n",
    "# accuracy_score(model3.predict(m.X_mean.value[3500:4500]),y_train[3500:4500])\n",
    "\n",
    "# model4=MultinomialNB().fit(np.array(m.X_mean.value[:3500]),y_train[:3500])\n",
    "# accuracy_score(model2.predict(np.array(m.X_mean),y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import GPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from GPy.models import BayesianGPLVM, DPBayesianGPLVM, GPLVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from GPy.core.parameterization.priors import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disPrior = GPy.core.parameterization.priors.DGPLVM_KFDA( 1, 1, y_trainred, GPy.kern.Linear ,x_trainred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = DPBayesianGPLVM(Y = Y,input_dim= 5, X_prior=GPy.core.parameterization.priors.Uniform(0,1), num_inducing=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import GPy\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import GPy # import GPy package\n",
    "np.random.seed(12345)\n",
    "GPy.plotting.change_plotting_library('plotly')\n",
    "Y = x_trainred.toarray()\n",
    "Y = Y.astype(float)\n",
    "Q = 20\n",
    "m_gplvm = GPy.models.GPLVM(Y, Q, kernel=GPy.kern.RBF(Q))\n",
    "m_gplvm.kern.lengthscale = .2\n",
    "m_gplvm.kern.variance = 1\n",
    "m_gplvm.likelihood.variance = 1.\n",
    "#m2.likelihood.variance.fix(.1)\n",
    "m_gplvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m_gplvm.optimize(messages=1, max_iters=5e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure = GPy.plotting.plotting_library().figure(1, 2, \n",
    "                        shared_yaxes=True,\n",
    "                        shared_xaxes=True,\n",
    "                        subplot_titles=('Latent Space', \n",
    "                                        'Magnification',\n",
    "                                        )\n",
    "                            )\n",
    "\n",
    "canvas = m_gplvm.plot_latent(labels=y_trainred, figure=figure, col=(1), legend=False)\n",
    "canvas = m_gplvm.plot_magnification(labels=y_trainred, figure=figure, col=(2), legend=False)\n",
    "\n",
    "GPy.plotting.show(canvas, filename='wishart_metric_notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import GPy\n",
    "GPy.plotting.change_plotting_library('matplotlib')\n",
    "m = GPy.models.bayesian_gplvm_minibatch.BayesianGPLVMMiniBatch(Y, input_dim=5, num_inducing=30, missing_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.optimize(messages=1, max_iters=5e1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import GPy\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import GPy # import GPy package\n",
    "np.random.seed(12345)\n",
    "GPy.plotting.change_plotting_library('plotly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, axe = plt.subplots(1, 2, figsize=(15,5))\n",
    "axe[0].set_title('Latent Space')\n",
    "v1 = m.plot_latent(labels=y_trainred, which_indices=[0,1])\n",
    "axe[1].set_title('Magnification')\n",
    "v2 = m.plot_magnification(labels=y_trainred, resolution=120, which_indices=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (type(disPrior))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "48684f5c909341cd810926074c9f351a": {
     "views": [
      {
       "cell_index": 25
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
